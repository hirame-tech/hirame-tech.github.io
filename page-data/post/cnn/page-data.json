{"componentChunkName":"component---src-templates-post-js","path":"/post/cnn/","result":{"pageContext":{"post":{"title":"CNNについてザックリと知る(解説編)","image":{"file":{"url":"//images.ctfassets.net/nxx3igvspvb5/5CSF4EC3A8DAbcR16b4KPr/36546b4d4d26e62ce774af1eae1eb712/___________________________.drawio.png"}},"body":{"childMarkdownRemark":{"html":"<p>みなさん、CNN(Convolutional Neural Network,畳み込みニューラルネットワーク)について知ってますでしょうか。今回は簡単なCNNの実装を通してCNNをザックリと知っていこうというお話です。</p>\n<h2>概要</h2>\n<hr>\n<p>pytorchとMNISTを用いた手書き文字認識CNNをgoogleColab上で作成し、それを通じてCNNについて知る。</p>\n<h2>CNNって何？</h2>\n<hr>\n<p>まず大きな括りとして、みなさんご存じAI(Artificial Intelligence)があります。\nこれの一種として機械学習(Machine Learning)が存在します。\nこの機械学習の手法の一種としてCNNが存在します。</p>\n<p>機械学習とは何らかのデータを用意し、これを学習し何らかのタスクをこなすものです。</p>\n<h2>導入１　Neural Networkと活性化関数</h2>\n<hr>\n<p>NeuralNetworkとは、人間の脳の神経細胞とそのつながりを数式的モデルで表現したもので、機械学習では、ノード(数値が格納されるところ)の集まりとしてレイヤー、レイヤーが何層にも積み重なってネットワークが構成されています。</p>\n<p><img src=\"//images.ctfassets.net/nxx3igvspvb5/gD8SOfi1bBtzQ5TTOoOsD/ea6295f2f916556c58a78c2c76dd1f9f/___________________________.drawio__1_.png\" alt=\"Network\"></p>\n<p>各ノードの枝には重みづけ(スカラー)がされていて、\n(次のノードに渡す値) = (重み) × (ノードの値)\nという風に積をとって次のノードに渡していきます。渡された側のノードは各枝から受け取った値の和をとって、格納します。これはレイヤー全体でみると、入力ベクトルxと重み行列Wを用いて、</p>\n<div class=\"gatsby-highlight\" data-language=\"math\"><pre class=\"language-math\"><code class=\"language-math\">  u_1 = Wx</code></pre></div>\n<p>と表されます。\nまた、活性化関数とは重み付けされた値を入力としてそのニューロンの出力を決める関数です。先の例では簡単のため重み付けのみで説明しましたが、実際のところこの重み付けしたものを活性化関数に入力し、その出力をニューロンの出力値とします。活性化関数は一般に非線形関数とすることが多く、シグモイド関数、ReLU等があります。</p>\n<h2>導入２　損失関数</h2>\n<hr>\n<p>学習させるにあたって、どのように変化させるべきか、どういった状況が結果の確度が高いかというのを把握する必要があります。これを計算するのが損失関数(loss function)です(端的に言うと、「正解値」と「予測値」のズレを計算する関数です)。ここでは交差エントロピー誤差を用います。</p>\n<h2>導入３　最適化関数</h2>\n<hr>\n<p>先で損失関数についての説明をしましたが、機械学習における目標とは、損失(予測値のズレ)をこの化することです。この目標をよりべく効率よく達場合にくれるのが最適化関数(optimizer)です。簡単に説明すると、ある重みwを変数として(学習とは重みを変化させて調整することであるので)変化する損失f(w)と人間が定義するパラメータa(正の定数)を考えたとき、</p>\n<div class=\"gatsby-highlight\" data-language=\"math\"><pre class=\"language-math\"><code class=\"language-math\">  w_next ← w_now - a \\frac{\\partial f(w)}{partial w}</code></pre></div>\n<p>という重みの更新を行います。つまり、重みを変化させて損失がどのように変化するか(微分)に係数を掛けたものを引いていくことで更新していきます。微分係数が負の場合、損失は小さくなるのでwは増やすべきで、微分係数が正の場合、損失は大きくなってしまうのでwは減らすべきといった風に変化させます(2次関数を考えるとわかりやすいかも？)。\nここでは確率的勾配降下法(SGD=Stochastic Gradient Descent)を用います。</p>\n<h2>導入４　誤差逆伝播法</h2>\n<hr>\n<p>さて、先で微分係数を求める話をしましたが、その微分係数を求めるアルゴリズムが、誤差逆伝播法(Backpropagation)です。\n高校で数学Ⅲを学んだ方なら合成関数の微分というものを習ったことかと思います。\nイメージとしてはz = f(g(x))という式について考えるとき、y = g(x)と置くとまずdz/dyを計算しその後dy/dzを計算していくという手順です。</p>\n<p>この誤差逆伝播法の利点は入力層より出力層が少ない場合に順伝播法より計算量が少なく済むことです(計算結果の再利用が可能なため)。順伝播法とは入力側から微分していく方法です。\n順伝播法と逆伝播法の違いの例としてz = f(g(x,y))について考えます。\n順伝播法で計算すると、項の順番が処理の順番と一致するとするならば</p>\n<div class=\"gatsby-highlight\" data-language=\"math\"><pre class=\"language-math\"><code class=\"language-math\">\\frac{\\partical z}{partial x} = \\frac{\\partical g}{partial x} \\frac{\\partical z}{partial g}\n\n\\frac{\\partical z}{partial y} = \\frac{\\partical g}{partial y} \\frac{\\partical z}{partial g}</code></pre></div>\n<p>と記述できます。同様に逆伝播法で計算すると、</p>\n<div class=\"gatsby-highlight\" data-language=\"math\"><pre class=\"language-math\"><code class=\"language-math\">\\frac{\\partical z}{partial x} = \\frac{\\partical z}{partial g} \\frac{\\partical g}{partial x}\n\n\\frac{\\partical z}{partial y}　= \\frac{\\partical z}{partial g} \\frac{\\partical g}{partial y}</code></pre></div>\n<p>と記述できます。つまり入力層が出力層より大きいとき、次第に経路が共通化していきます(上の式でいうと∂z/∂gの部分)。今回は共通部分は一経路となりますが、共通部分が大きくなったとき、逆伝播法だと共通部分の積を先に計算していくことになります。すると分岐するまでは一回の計算で済みます。順伝播法では共通部分は後に計算する事になりますので共通部分のみの積はわからないため、素直に計算する事になります。\nよってこのときは逆伝播法のほうが計算が少ない、という結論になります。</p>\n<h2>Convolutionalの意味</h2>\n<hr>\n<p>本題に入る前に、当初はこの章の前にプログラムの実装の章を置いていたので以下を読み進める前に実装編に移ってもいいかもしれません(私は座学をほどほどに手を動かしながら進めるほうが頭にはいるタイプなので)。</p>\n<p>では話を元に戻していきましょう。Convolutionalの意味、ということですがこの語はCNNのCの部分にあたる語で日本語では「畳み込み」です。ならCNN、すなわち畳み込みニューラルネットワークってどういう種類のニューラルネットワークだ？という疑問に行き着くわけです。\nまず畳み込みについて、ここでは離散値について話します。</p>\n<h2>交差エントロピー誤差</h2>\n<h2>確率的勾配降下法</h2>"}},"createdAt":"2023年04月20日","description":{"description":"CNNについての理解を深める(解説編)"},"metadata":{"tags":[]},"slug":"cnn"}}},"staticQueryHashes":["2841359383","426988268"],"slicesMap":{}}